# -*- coding: utf-8 -*-
"""[TEMPLATE] GMA FINDER v1.7

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CtFJspoRzLxTUZOfivYvi_qkRItZhR_l

<img src="https://drive.google.com/uc?id=1BvpJalg9KNSSH9TXFFcAyppDDbe-Dskl" height="40px" align="left" hspace="10px" alt="New"></img>

# **GMA Finder**

**Goal**:
Process the spend and revenue data provided by stakeholders to obtain a valid input that works in Google Geolift collabs.

**Data Needed**:
- historical revenue by location (e.g. city, zipcode, region etc.)
- historical spend by location (e.g. city, zipcode, region etc.)
- Optionnal: assignment table with "geo" and "assignment" columns
---

**Example 1**:

[Example 1 - Input - historical revenue by location](https://docs.google.com/spreadsheets/d/1_tdsgyfBYYiLrnKPC9KH7s49ned-936W1Uu9-FQQCBY/edit#gid=0)\
[Example 1 - Input - historical spend by location](https://docs.google.com/spreadsheets/d/1Uuc0L5-A1m8xSl_FRGVxj7v2RZxl3FHxMeuLmBnBGIY/edit#gid=260701259)\
[Example 1 - Output](https://docs.google.com/spreadsheets/d/1CcFAAIzC9kgXrEpIi_PqBF5Fpek7YQxkRZt-1OanJMo/edit#gid=701843571)

**Example 2**:

[Example 2 - Input - historical revenue by location](https://docs.google.com/spreadsheets/d/1q4ZoKCgDwJkIcrHFve2hjOZUPeOVUhh73Ow2iiMwMaM/edit#gid=1020803697)\
[Example 2 - Input - historical spend by location](https://docs.google.com/spreadsheets/d/1b4RrmOGi1YsIkLOBf_hrpKCkxEWDF3lPH7qGiLPwtac/edit#gid=572917173)\
[Example 2 - Output](https://docs.google.com/spreadsheets/d/1zRbrBIj5cnytmHVkYAPySyM2M017TMPnymAIrdvpxUc/edit#gid=772799190)

# **Notes to improve template**

***To do***:
*   Data processing:
 * remove "," in "float columns" ✔
 *   *Add % spend/revenue for excluded geos (for post analysis output)* ✔
 *   *Add description of Filter & Interpolation methods + add thresholds selector option?*
 *   *Handle errors: Add a read_data() function that returns False if data could not be read*
 *   *Handle errors: Replace the try/except statements with ifs based on what read_data() returned*
 *   *Handle case when different levels of location in revenue vs. spend data*
 *   *Add date format management*
*   *Improve descriptive stats*
*   Clean code:
 * use modules, rewrite/optimize for better code reading, check comments
*  *GMA data: table up to date?*
---
***Notes from [Design Doc Bissell (Amazon DSP STV).doc](https:https://docs.google.com/document/d/1Ea1U9sQIe0JZgXoTr0mZ9PhbHSB4NmoDNH78xRp4AFk/edit#//)***:
* All test design files are named with prefix “Design_” ✔
* All experiment files are named with prefix “Test_” ✔
* Preliminary Exploration on Raw Data: Spend TS/Revenue TS ✔
* Exploratory data analysis:
 * Spend time series ✔
 * Top 5 DMAs with the highest average spend ✔
 * Bottom 5 DMAs in average spend ✔
 * Top 5 DMAs with the highest total spend ✔
 * Bottom 5 DMAs in total spend ✔

---
<img src="https://drive.google.com/uc?id=1AJnSLScn3kSrOLPZGONcfIJ4m8ZnyHmR" height="50px" align="left" hspace="4px" alt="New"></img>
# **I. INITIAL CONNEXION**
---
"""

from google.colab import auth
auth.authenticate_user()

mydate = "2023-02-20" #@param {type:"date"}
phase ='Design' #@param ["Design","Test"]
da_name = 'Gregoire Lecallier' #@param ["Louise Lacour", "Leonard Trebern", "Sebastien Benferhat", "Thibault Martin", "Younes Lahouir", "Patrick Lin", "Joo Yoon", "Gregoire Lecallier","Sara Loew"]
adv_name = 'YR'  #@param {type:"string"}
DAN = 'test' #@param {type:"string"}
details = 'metagoogle'  #@param {type:"string"}

da_name = da_name[0].lower()+da_name[1:].replace(' ','')
adv_name = adv_name.replace('_','')
details = details.replace('_','')
mydate = mydate.replace("-", "")

analysis_name = phase + '_geodataprocess_'
#TITLE TABLE
tablename = (analysis_name+adv_name+'_'+da_name+'_DAN'+DAN+'_'+mydate+'_'+details).lower()
print('[TABLE TITLE]')
print(tablename)

"""---
<img src="https://drive.google.com/uc?id=1AJnSLScn3kSrOLPZGONcfIJ4m8ZnyHmR" height="50px" align="left" hspace="4px" alt="New"></img>
# **II. INSTALL LIBRARIES**
---
"""

#@title **a. Storage files**
#@markdown *APIs token file and ADH account dictionnary*
#%%capture
token_file = 'apis_token_automatisation_'+da_name+'.txt'
!gsutil cp gs://bkt-dataproc-prod-eu-da-ba/adh/adh_automatisation/team_token/{token_file} apis_token_automatisation.txt

#@markdown *DA utils repository*
!gsutil cp gs://bkt-dataproc-prod-eu-da-ba/adh/adh_automatisation/utils_repository/apis_connection_utils.py .
!gsutil cp gs://bkt-dataproc-prod-eu-da-ba/adh/adh_automatisation/utils_repository/gsheet_utils.py .

# Commented out IPython magic to ensure Python compatibility.
# #@title **b. Libairies**
# %%capture
# 
# import sys
# sys.path.append('/content/')
# 
# from apis_connection_utils import *
# from gsheet_utils import *
# 
# #@markdown  *Install initial library*
# !pip install -U -q PyDrive
# !pip install -q xlrd
# !pip install fuzzywuzzy
# 
# import gspread
# import warnings
# from google import auth as google_auth
# from google.colab import auth
# from google.colab import data_table
# from google.colab import widgets
# from google.colab import drive
# import io
# import zipfile
# from googleapiclient.http import MediaIoBaseDownload
# from googleapiclient.discovery import build
# 
# import pandas as pd
# import numpy as np
# import math
# import datetime as dt
# import warnings
# import ipywidgets as widgets
# from ipywidgets import interact, interactive, fixed, interact_manual
# from ipywidgets import FloatSlider
# warnings.simplefilter(action='ignore', category=FutureWarning)
# 
# import seaborn as sns
# pd.options.mode.chained_assignment = None
# #figure style
# sns.set_style("darkgrid", {'axes.axisbelow': False})
# #figure size
# sns.set(rc = {'figure.figsize':(11,5)})
# import matplotlib.pylab as plt
# import matplotlib.image as mpimg
# 
# import scipy.stats as stats
# import statsmodels.tsa.stattools as stattools
# from fuzzywuzzy import fuzz
# 
# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials
# 
# auth.authenticate_user()
# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)

# Commented out IPython magic to ensure Python compatibility.
# #@title **c. APIs connexion**
# %%capture
# keys = dict()
# with open('apis_token_automatisation.txt','r') as f :
#     for row in f :
#         k = row.split('=')
#         keys.update({k[0].strip():k[1].strip()})
# 
# CLIENT_ID = [v for k,v in list(keys.items()) if k == 'CLIENT_ID'][0]
# CLIENT_SECRET = [v for k,v in list(keys.items()) if k == 'CLIENT_SECRET'][0]
# REFRESH_TOKEN = [v for k,v in list(keys.items()) if k == 'REFRESH_TOKEN'][0]
# DEVELOPPER_KEY  = [v for k,v in list(keys.items()) if k == 'DEVELOPPER_KEY'][0]
# 
# service_sheet = connect_apis('sheet', DEVELOPPER_KEY, CLIENT_ID, CLIENT_SECRET, REFRESH_TOKEN )

#@title **d. Specific function**

#function used in part III. c. to import gma data from Postcode Mappings.zip file
def download_csv_from_zip(country: str):
  drive_service = build('drive', 'v3')
  zip_file_id = "1NMM2PwGYjLjd48YKTYXll4zwIyyJtMaV"
  request = drive_service.files().get_media(fileId=zip_file_id)
  downloaded = io.BytesIO()
  downloader = MediaIoBaseDownload(downloaded, request)
  done = False

  while done is False:
      status, done = downloader.next_chunk()
      if status:
          print("Download %d%%." % int(status.progress() * 100))

  downloaded.seek(0)

  # Open the downloaded zip file
  with zipfile.ZipFile(downloaded, 'r') as zip_file:
      # Get a list of all file names in the zip file
      file_names = zip_file.namelist()

      # Filter the file names to only include csv files that contain the specified country
      csv_files = [file_name for file_name in file_names if file_name.endswith('.csv') and country in file_name]

      # Check that a matching CSV file was found
      if len(csv_files) == 0:
          print(f"No CSV file found for country {country}")
      else:
          # Extract the first matching CSV file
          csv_file_name = csv_files[0]
          with zip_file.open(csv_file_name, 'r') as csv_file:
              # Read the CSV data into a pandas DataFrame
              gma_data = pd.read_csv(csv_file,dtype=str)
  return gma_data

#function used in part IV. b. to handle NaN
def fill_missing_data(freq=["D","W"], method = ["None","Filter", "Interpolation"], threshold =0, plot = [False, True]):
  if freq == "D":
    print_freq = "Daily"
  else:
    print_freq = "Weekly"

  if method == "Filter":
    print('\033[1m' +'Before Geo Filtering:' + '\033[0;0m')
    print("\n")
    try:
      print("%s Mean Cost by geo: %s" %(print_freq,round(merged_data["cost"].mean(),2)))
      print("%s Median Cost by geo: %s" %(print_freq,round(merged_data["cost"].median(),2)))
    except:
      pass
    print("%s Mean response by geo: %s" %(print_freq,round(merged_data["response"].mean(),2)))
    print("%s Median response by geo: %s" %(print_freq,round(merged_data["response"].median(),2)))

    print("____________________________________")
    print("\n")

    print('\033[1m' + 'Geo Filtering:' + '\033[0;0m')
    print("\n")
    #threshold = 0.5
    excluded_geo = []

    for geo in merged_data['geo'].unique():
      data_geo = merged_data.loc[(merged_data['geo'] == geo)]
      missing_ratio = len(data_geo.loc[data_geo["response"]>0])/len(data_geo)
      if missing_ratio < threshold:
        excluded_geo.append(geo)

    processed_merged_data = merged_data.loc[~merged_data["geo"].isin(excluded_geo)]
    #processed_merged_data.columns = ['date' if c == date_col else 'geoName' if c == geo_col2 else 'geo' if c == 'Gma' else 'response' if c == response_col else 'cost' if c == spend_col else c for c in processed_merged_data.columns]

    try:
      processed_merged_data = pd.merge(processed_merged_data,assignment_table[['geo','assigment']],how="left",on="geo")
      processed_merged_data = processed_merged_data.loc[processed_merged_data['assigment'].notnull()]
    except:
      pass
    try:
      print("%s Mean Cost: %s" %(print_freq,round(processed_merged_data['cost'].mean(),2)))
      print("%s Median Cost: %s" %(print_freq,round(processed_merged_data['cost'].median(),2)))
    except:
      pass
    print("%s Mean response: %s" %(print_freq,round(processed_merged_data['response'].mean(),2)))
    print("%s Median response: %s" %(print_freq,round(processed_merged_data['response'].median(),2)))

    print("\n")

    print("Number of excluded geos: ")
    print(len(excluded_geo))
    print("Number of geos in treated merged data: ")
    print(processed_merged_data["geo"].nunique())

    print("\n")

    print("Share of response present in merge dataframe:")
    print(round(processed_merged_data['response'].sum() / init_revenue,2))
    print("Share of media present in merge dataframe:")
    try:
      print(round(processed_merged_data['cost'].sum() / init_spend,2))
    except:
      pass

    print("\n")

  elif method == "Interpolation" and (missing_spend == False or missing_revenue == False):
    print('\033[1m' + 'Before Interpolation:' + '\033[0;0m')
    print("\n")
    try:
      print("%s Mean Cost: %s" %(print_freq,round(merged_data["cost"].mean(),2)))
      print("%s Median Cost: %s" %(print_freq,round(merged_data["cost"].median(),2)))
    except:
      pass
    try:
      print("%s Mean response: %s" %(print_freq,round(merged_data["response"].mean(),2)))
      print("%s Median response: %s" %(print_freq,round(merged_data["response"].median(),2)))
    except:
      pass

    print("____________________________________")
    print("\n")

    print('\033[1m' + 'Interpolation of missing data' + '\033[0;0m')
    threshold = 1 - threshold
    excluded_geo = []
    try:#create empty processed data with same columns as merged data
      #processed_merged_data = pd.DataFrame(columns=merged_data.columns)
      processed_merged_data = merged_data.head(0).copy()
    except:
      pass
    try: #try to assign control/treatment with assignment table
      processed_merged_data = pd.merge(processed_merged_data,assignment_table[['geo','assigment']],how="left",on="geo")
      processed_merged_data = processed_merged_data.loc[processed_merged_data['assigment'].notnull()]
    except:
      pass

    for geo in merged_data['geo'].unique():
      data_geo = merged_data.loc[(merged_data['geo'] == geo)]
      if round(data_geo['response'].isna().sum() / data_geo.shape[0],2) > threshold:
        excluded_geo.append(geo)
      else:
        data_geo['response_old'] = data_geo['response']
        #data_geo['response'] = data_geo['response'].interpolate(method='spline', order=3)
        data_geo['response'] = data_geo['response'].interpolate(method='polynomial', order=1)
        processed_merged_data = pd.concat([processed_merged_data, data_geo], join="inner")
        if round(data_geo['response_old'].isna().sum() / data_geo.shape[0],2) > 0.1:
          ax = data_geo.plot(x='date', y='response', legend=False, color = "r",label='Interpolated Response')
          data_geo.plot(x='date', y='response_old', ax=ax, legend=False, color="royalblue",label='Response') #mediumslateblue thistle mediumpurple orchid blueviolet
          ax.figure.legend()
          # displaying the title
          plt.title("Response & Cost - Geo: " + str(geo))
          plt.show()

    print("\n")

    try:
      print("%s Mean Cost: %s" %(print_freq,round(processed_merged_data['cost'].mean(),2)))
      print("%s Median Cost: %s" %(print_freq,round(processed_merged_data['cost'].median(),2)))
    except:
      pass
    print("%s Mean response: %s" %(print_freq,round(processed_merged_data['response'].mean(),2)))
    print("%s Median response: %s" %(print_freq,round(processed_merged_data['response'].median(),2)))

    print("\n")

    print("Number of excluded geos: ")
    if len(excluded_geo) > 0:
      additional_print  = " (use user_form.result[1] to see the list of excluded geos)"
    else:
      additional_print = ""

    print(str(len(excluded_geo)) + additional_print)
    print("Number of geos in treated merged data: ")
    print(processed_merged_data["geo"].nunique())

    print("\n")

    print("Share of response present in merge dataframe:")
    print(round(processed_merged_data['response'].sum() / init_revenue,2))
    try:
      print("Share of media present in merge dataframe:")
      print(round(processed_merged_data['cost'].sum() / init_spend,2))
    except:
      pass
    print("\n")
  elif method == "Interpolation" and (missing_spend == True and missing_revenue == True):
    print("\n")
    print("Cannot perform Interpolation to fill missing data because spend data and revenue data are missing...")
    print("\n")
  else:
    excluded_geo = []
    processed_merged_data = merged_data.copy()
    try:
      processed_merged_data = pd.merge(processed_merged_data,assignment_table[['geo','assigment']],how="left",on="geo")
      processed_merged_data = processed_merged_data.loc[processed_merged_data['assigment'].notnull()]
    except:
      pass

  #processed_merged_data = processed_merged_data.fillna(0)

  #ploting data:
  if plot == True:
    print('\033[1m' +'Pair plot:' + '\033[0;0m')
    try:
      g0 = sns.pairplot(merged_data.groupby([pd.Grouper(key="date", freq=freq)]).sum().reset_index(), height=3.5, aspect=1)
    except:
      g0 = sns.pairplot(merged_data.groupby([pd.Grouper(key="date")]).sum().reset_index(), height=3.5, aspect=1)
    g0.map_diag(sns.histplot, color = "#3182bd")
    g0.map_offdiag(sns.scatterplot, color = "#3182bd")
    g0.fig.suptitle("Pairplot before processing", y=1.02)

    try:
      g1 = sns.pairplot(processed_merged_data.groupby([pd.Grouper(key='date', freq=freq)]).sum().reset_index(), height=3.5, aspect=1)
    except:
      g1 = sns.pairplot(processed_merged_data.groupby([pd.Grouper(key='date')]).sum().reset_index(), height=3.5, aspect=1)
    g1.map_diag(sns.histplot, color = "#08519c")
    g1.map_offdiag(sns.scatterplot, color = "#08519c")
    g1.fig.suptitle("Pairplot after processing", y=1.02)

    ############### 1. SAVE PLOTS IN MEMORY TEMPORALLY
    g0.savefig('g0.png', dpi=300)
    plt.close(g0.fig)

    g1.savefig('g1.png', dpi=300)
    plt.close(g1.fig)


    ############### 2. CREATE YOUR SUBPLOTS FROM TEMPORAL IMAGES
    f, axarr = plt.subplots(1, 2, figsize=(20, 9))

    axarr[0].imshow(mpimg.imread('g0.png'))
    axarr[1].imshow(mpimg.imread('g1.png'))

    # turn off x and y axis
    [ax.set_axis_off() for ax in axarr.ravel()]

    plt.tight_layout()
    plt.show()

    print("\n")
    print('\033[1m' +'Time Series:' + '\033[0;0m')

    num_cols = merged_data.select_dtypes(include=['int','float']).columns
    if len(num_cols) / 3 < 1:
      cols = len(num_cols)
    else:
      cols = 3
    rows = math.ceil(len(num_cols) / 3)

    fig = plt.figure( figsize=(cols*8, rows*5))
    for i, col in enumerate(num_cols):
      ax=fig.add_subplot(rows,cols,i+1)
      sns.lineplot(data=merged_data.groupby([pd.Grouper(key="date", freq=freq)]).sum().reset_index(), x="date", y=merged_data.groupby([pd.Grouper(key="date", freq=freq)]).sum().reset_index()[col], ax = ax,color='#6baed6',linestyle='--')
      sns.lineplot(data=processed_merged_data.groupby([pd.Grouper(key="date", freq=freq)]).sum().reset_index(), x="date", y=processed_merged_data.groupby([pd.Grouper(key="date", freq=freq)]).sum().reset_index()[col], ax = ax,color='#08519c')
    fig.tight_layout()
    plt.legend(labels=["Before processing","After processing"], bbox_to_anchor = (-0.6,1.2))
    plt.show()

  print("\n")
  print('\033[1m' +'Data:' + '\033[0;0m')
  print("\n")
  print(processed_merged_data.head(3).to_markdown())
  return processed_merged_data, excluded_geo

"""---
<img src="https://drive.google.com/uc?id=1AJnSLScn3kSrOLPZGONcfIJ4m8ZnyHmR" height="50px" align="left" hspace="4px" alt="New"></img>
# **III. IMPORT DATA**
---
"""

#@title **a. Import media data**
#@markdown Spreadsheet URL containing the spend data (optionnal: sheet_name)
url_media_data =  "" #@param {type:"string"}
sheet_name = ""#@param {type:"string"}
creds, _ = google_auth.default()
gc = gspread.authorize(creds)
try:
  if sheet_name == "" and url_media_data.find('/file/') == -1: #if sheet_name not provided and URL is a gsheet URL
    wks = gc.open_by_url(url_media_data).sheet1
  elif sheet_name == "" and url_media_data.find('/file/') > -1: #if file provided instead of gsheet URL
    import io
    from googleapiclient.http import MediaIoBaseDownload
    from googleapiclient.discovery import build

    file_id = url_media_data.split("/")[5]
    drive_service = build('drive', 'v3')
    request = drive_service.files().get_media(fileId=file_id)
    downloaded = io.BytesIO()
    downloader = MediaIoBaseDownload(downloaded, request)
    done = False
    while done is False:
       _, done = downloader.next_chunk()
    downloaded.seek(0)
  else: #opens gsheet url using the sheet_name provided
    wks = gc.open_by_url(url_media_data).worksheet(sheet_name)

  if url_media_data.find('/file/') == -1:
    media_data = wks.get_all_values()
    headers = media_data.pop(0)
    media_data = pd.DataFrame(media_data, columns=headers)
  else:
    media_data = pd.read_csv(downloaded)
  #@markdown Enter name of the Date column as in the gsheet:
  date_col = "" #@param {type:"string"}
  dayfirst = False #@param {type:"boolean"}
  try:
    media_data[date_col] = pd.to_datetime(media_data[date_col],dayfirst=dayfirst)
  except:
    pass
  #@markdown Enter name of the Spend column as in the gsheet:
  spend_col = "" #@param {type:"string"}
  format = '1234.56'#@param ["1234.56", "1,234.56", "1234,56"]
  #@markdown Enter name of the geo level column as in the gsheet:
  geo_col = "" #@param {type:"string"}

  media_data[geo_col] = media_data[geo_col].astype(object)
  #removing currency symbols and converting oter columns to numeric if possible:
  for col in media_data.drop([date_col,geo_col], axis=1, errors='ignore').columns:
    if col[-2:].lower() != "id": #do nothing if colname ends with "id" => supposedly an id col
      #replace decimal sep according to format selected:
      if format == "1234,56":
        media_data[col] = media_data[col].str.replace(",",".")
      elif format == "1,234.56":
        media_data[col] = media_data[col].str.replace(",","")
      #replace currency symbols to allow numeric conversion
      try:
        media_data[col] = media_data[col].str.replace("US$","").str.replace("$","").str.replace("US","").str.replace("€","").str.replace("EU","")
      except:
        pass
      #try to convert to numeric
      try:
        media_data[col] = pd.to_numeric(media_data[col])
      except:
        pass

  #data frame description:
  init_spend = media_data[spend_col].sum()
  print("%s rows and %s columns loaded"%(media_data.shape[0],media_data.shape[1]))
  try:
    print("From %s"%(media_data[date_col].min()))
    print("To %s"%(media_data[date_col].max()))
  except:
    pass

  print("\n")
  print("Dtypes:")
  print("—-—-—-—-—-—-—-——-—-—-—-—-—-—-—--—-—-—-—-")
  print(media_data.dtypes)
  print("\n")
  print(media_data.head(3).to_markdown())
except:
  date_col = ""
  spend_col = ""
  geo_col = ""

#@title **b. Import revenue data**
#@markdown Spreadsheet URL containing the revenue data (optionnal: sheet_name)
url_response_data =  "https://drive.google.com/file/d/1nU8FXPpMaoXxJveqhrv7ruOoXqalPdV0/view?usp=sharing" #@param {type:"string"}
sheet_name = ""#@param {type:"string"}
creds, _ = google_auth.default()
gc = gspread.authorize(creds)
try:
  if sheet_name == "" and url_response_data.find('/file/') == -1: #if no sheet_name provided
    wks = gc.open_by_url(url_response_data).sheet1
  elif sheet_name == "" and url_response_data.find('/file/') > -1:
    import io
    from googleapiclient.http import MediaIoBaseDownload
    from googleapiclient.discovery import build

    file_id = url_response_data.split("/")[5]
    drive_service = build('drive', 'v3')
    request = drive_service.files().get_media(fileId=file_id)
    downloaded = io.BytesIO()
    downloader = MediaIoBaseDownload(downloaded, request)
    done = False
    while done is False:
       _, done = downloader.next_chunk()
    downloaded.seek(0)
  else: #opens url using the sheet name provided
    wks = gc.open_by_url(url_response_data).worksheet(sheet_name)

  if url_response_data.find('/file/') == -1:
    revenue_data = wks.get_all_values()
    headers = revenue_data.pop(0)
    revenue_data = pd.DataFrame(revenue_data, columns=headers)
  else:
    revenue_data = pd.read_csv(downloaded)

  #@markdown Enter name of the Date column as in the gsheet:
  date_col2 = "date" #@param {type:"string"}
  dayfirst = True #@param {type:"boolean"}
  try:
    revenue_data[date_col2] = pd.to_datetime(revenue_data[date_col2],dayfirst=dayfirst)
  except:
    pass
  #@markdown Enter name of the Response column as in the gsheet:
  response_col = "CA_retail" #@param {type:"string"}
  format = '1234.56'#@param ["1234.56", "1,234.56", "1234,56"]
  #@markdown Enter name of the geo level column as in the gsheet:
  geo_col2 = "code_postal_client" #@param {type:"string"}

  revenue_data[geo_col2] = revenue_data[geo_col2].astype(object)
  #removing currency symbols and converting oter columns to numeric if possible:
  for col in revenue_data.drop([date_col2,geo_col2], axis=1, errors='ignore').columns:
    if col[-2:].lower() != "id": #do nothing if colname ends with "id" => supposedly an id col
      #replace decimal sep according to format selected:
      if format == "1234,56":
        revenue_data[col] = revenue_data[col].str.replace(",",".")
      elif format == "1,234.56":
        revenue_data[col] = revenue_data[col].str.replace(",","")
      #replace currency symbols to allow numeric conversion
      try:
        revenue_data[col] = revenue_data[col].str.replace("US$","").str.replace("$","").str.replace("US","").str.replace("€","").str.replace("EU","")
      except:
        pass
      #try to convert to numeric
      try:
        revenue_data[col] = pd.to_numeric(revenue_data[col])
      except:
        pass

  #data frame description:
  init_revenue = revenue_data[response_col].sum()
  print("%s rows and %s columns loaded"%(revenue_data.shape[0],revenue_data.shape[1]))
  try:
    print("From %s"%(revenue_data[date_col2].min()))
    print("To %s"%(revenue_data[date_col2].max()))
  except:
    pass
  print("\n")
  print("Dtypes:")
  print("—-—-—-—-—-—-—-——-—-—-—-—-—-—-—--—-—-—-—-")
  print(revenue_data.dtypes)
  print("\n")
  print(revenue_data.head(3).to_markdown())
except:
  pass

#@title **c. Import GMA data**
#@markdown Which GMA data would you like to use?
chose = 'Postcode Mappings' #@param ["GMA_all_countries - GMA mapping merged for all countries","Postcode Mappings"]
#@markdown Enter geo level of the response and spend data:
geo_level = 'PostCode' #@param ["City","GeoCid","PostCode" ,"GmaName","Gma"]
if chose == 'Postcode Mappings' and geo_level != "PostCode":
  print("Warning: you can only use PostCode with Postcode Mappings")

#@markdown *You can set fuzzy_matching to Yes if geo_level is a string (like address, city...):*
fuzzy_matching = 'No' #@param ["Yes", "No"]

#@markdown *Select "All" if there are multiple countries:*
#country = 'FR'#@param ["All", 'AT', 'AU', 'BE', 'BR', 'CA', 'CH', 'DE', 'DK', 'ES', 'FR', 'GB','IN', 'IT', 'JP', 'MX', 'NL', 'NO', 'RU', 'SE', 'US']
country =  'FR'#@param['AT','AU','BE','BR','CA','CH','DE','DK','ES','FR','GB','IN','IT','JP','MX','NL','NO','RU','SE','TR','US']

if geo_level != 'Gma' and chose == 'GMA_all_countries - GMA mapping merged for all countries':
  url_gma="https://docs.google.com/spreadsheets/d/1SbSh8DFgsvcf0edlsRwBKCX0AZb5_zictOrcez7wY5o/edit#gid=1740592113"
  wks = gc.open_by_url(url_gma).sheet1
  gma_data = wks.get_all_values()
  headers = gma_data.pop(0)
  gma_data = pd.DataFrame(gma_data, columns=headers)
elif  geo_level == 'Gma' and chose == 'GMA_all_countries - GMA mapping merged for all countries':
  print('You selected Gma to identify the Gmas in spend and revenue data: this means that the Gmas are already inside those datasets.')
  url_gma="https://docs.google.com/spreadsheets/d/1SbSh8DFgsvcf0edlsRwBKCX0AZb5_zictOrcez7wY5o/edit#gid=1740592113"
  wks = gc.open_by_url(url_gma).sheet1
  gma_data = wks.get_all_values()
  headers = gma_data.pop(0)
  gma_data = pd.DataFrame(gma_data, columns=headers)
else: #use zip file for gma data
  gma_data = download_csv_from_zip(country)

if country != "All" and chose == 'GMA_all_countries - GMA mapping merged for all countries':
  gma_data = gma_data.loc[gma_data["CountryCode"] == country]

if fuzzy_matching == "Yes" and chose == 'GMA_all_countries - GMA mapping merged for all countries':
  print("Minimum % of matching score to validate fuzzy match (100% = exact match):")
  slider = widgets.IntSlider(min=80,value=90, max=100)
  display(slider)
  sim_threshold_revenue = slider.value
  print("***Please check similarity scores in the gsheet output if you use fuzzy matching***")

#@title **d. Import Assignment table**

#@markdown *Columns in th gsheet should be named 'geo' and 'assigment'*

url_assignment_table =  "" #@param {type:"string"}
sheet_name = ""#@param {type:"string"}
creds, _ = google_auth.default()
gc = gspread.authorize(creds)
try:
  wks = gc.open_by_url(url_assignment_table).worksheet(sheet_name)
  assignment_table = wks.get_all_values()
  headers = assignment_table.pop(0)
  assignment_table = pd.DataFrame(assignment_table, columns=headers)
except:
  assignment_table = pd.DataFrame()

"""---
<img src="https://drive.google.com/uc?id=1AJnSLScn3kSrOLPZGONcfIJ4m8ZnyHmR" height="50px" align="left" hspace="4px" alt="New"></img>
# **IV. DATA PROCESSING**
---
"""

#@title **a. Merging data**

#@markdown Merging revenue and spend data ( ex: revenue.merge(spend, how = 'left') ):
join_type = 'left' #@param ["left","right","full" ,"inner"]

if geo_level != 'Gma': #if data does not already have matched GMAs:
  if fuzzy_matching == "Yes": #performs a fuzzy match to match GMAs based on geo_level similarities
    try:
      # Perform fuzzy string matching
      print("Finding GMA for revenue data:")
      print("Perform fuzzy string matching for Revenue...")
      tuples_list = [max([(fuzz.token_set_ratio(i,j),j) for j in gma_data[geo_level].unique()]) for i in revenue_data[geo_col2].unique()]
      # Unpack list of tuples into two lists
      print("Unpack list of tuples into two lists...")
      similarity_score, fuzzy_match = map(list,zip(*tuples_list))
      # Create pandas DataFrame
      df = pd.DataFrame({geo_col2:revenue_data[geo_col2].unique(), "GMA match": fuzzy_match, "similarity score":similarity_score})
      if geo_level != geo_col2:
        df = df.merge(gma_data[[geo_level, "Gma"]].drop_duplicates(), left_on='GMA match',right_on=geo_level, how='left')[[geo_col2, geo_level, "Gma", "similarity score"]]
      else:
        df = df.merge(gma_data[[geo_level, "Gma"]].drop_duplicates(), left_on='GMA match',right_on=geo_level, how='left').rename(columns = {geo_col2+"_x":geo_col2})[[geo_col2, "GMA match","Gma", "similarity score"]]
      print("build data...")
      revenue_data_gma = pd.merge(revenue_data, df,  how='left', on=[geo_col2])
      sim_threshold_revenue = slider.value
      revenue_data_gma = revenue_data_gma.loc[revenue_data_gma["similarity score"] > sim_threshold_revenue].drop(columns = ['similarity score'])
      print("{:.0%}".format(1 - len(df.loc[df['similarity score']<=sim_threshold_revenue]) / len(df)) + " of the geos found have a matched score of +{:.0%}".format(sim_threshold_revenue/100)+". Use df.loc[df['similarity score']<={}".format(sim_threshold_revenue)+"] to see the geos that have < {:.0%}".format(sim_threshold_revenue/100)+" similarity.")
      print("\n")
    except:
      pass

    try:
      print("Finding GMA for spend data:")
      print("Perform fuzzy string matching for Spend...")
      tuples_list2 = [max([(fuzz.token_set_ratio(i,j),j) for j in gma_data[geo_level].unique()]) for i in media_data[geo_col].unique()]
      # Unpack list of tuples into two lists
      print("Unpack list of tuples into two lists...")
      similarity_score, fuzzy_match = map(list,zip(*tuples_list2))
      # Create pandas DataFrame
      df2 = pd.DataFrame({geo_col:media_data[geo_col].unique(), "GMA match": fuzzy_match, "similarity score":similarity_score})
      if geo_level != geo_col:
        df2 = df2.merge(gma_data[[geo_level, "Gma"]].drop_duplicates(), left_on='GMA match',right_on=geo_level, how='left')[[geo_col, geo_level, "Gma", "similarity score"]]
      else:
        df2 = df2.merge(gma_data[[geo_level, "Gma"]].drop_duplicates(), left_on='GMA match',right_on=geo_level, how='left').rename(columns = {geo_col+"_x":geo_col})[[geo_col, "Gma", "similarity score"]]
      print("build data...")
      media_data_gma = pd.merge(media_data, df2,  how='left', on=[geo_col])
      sim_threshold_spend = slider.value
      media_data_gma = media_data_gma.loc[media_data_gma["similarity score"] > sim_threshold_spend].drop(columns = ['similarity score'])
      print("{:.0%}".format(1 - len(df2.loc[df2['similarity score']<=sim_threshold_spend]) / len(df2)) + " of the geos found have a matched score of +{:.0%}".format(sim_threshold_spend/100)+". Use df2.loc[df2['similarity score']<={}".format(sim_threshold_spend)+"] to see the geos that have < {:.0%}".format(sim_threshold_spend/100)+" similarity.")
      print("\n")
    except:
      pass

  else: #if fuzzy_match = "No":
    try: #try to match GMAs in revenue_data:
      print("Finding GMA for revenue data:")
      print("\n")
      #revenue_data_gma = revenue_data.merge(gma_data[[geo_level, "Gma"]].drop_duplicates(), left_on=geo_col2,right_on=geo_level, how='left')[[date_col2,response_col,geo_level,"Gma"]].groupby(["Date","Gma"]).agg([lambda x: 'first' if x.dtype == 'object' else 'sum'])
      revenue_data_gma = revenue_data.merge(gma_data[[geo_level, "Gma"]].drop_duplicates(), left_on=geo_col2,right_on=geo_level, how='left').groupby([date_col2,"Gma"]).sum().reset_index()
    except:
      pass
    try: #try to match GMAs in spend data:
      print("Finding GMA for spend data:")
      print("\n")
      #media_data_gma = media_data.merge(gma_data[[geo_level, "Gma"]].drop_duplicates(), left_on=geo_col,right_on=geo_level, how='left')[[date_col,spend_col,geo_level,"Gma"]].groupby(["Date","Gma"]).agg([lambda x: 'first' if x.dtype == 'object' else 'sum'])
      media_data_gma = media_data.merge(gma_data[[geo_level, "Gma"]].drop_duplicates(), left_on=geo_col,right_on=geo_level, how='left').groupby([date_col,"Gma"]).sum().reset_index()
    except:
      pass

  print("Merging spend and revenue data:")
  try:
      media_data_gma #check if media_data exists
  except: #if not:
    try:
        revenue_data_gma #check if revenue_data exists
    except NameError: #if not either:
      print("well, media and revenue data WEREN'T defined !")
      missing_spend = True
      missing_revenue = True
    else: #media_data does not exists & revenue_data exists:
      print("Seems like spend data was not defined: matching GMAs for revenue data only...")
      print("\n")
      missing_spend = True
      missing_revenue = False
      merged_data = revenue_data_gma.groupby([date_col2,'Gma']).sum().reset_index() #use only revenue_data

  else: #media_data exists:
      try:
        revenue_data_gma #check if revenue_data exists
      except NameError:
        print("Seems like revenue data was not defined: matching GMAs for spend data only...")
        print("\n")
        missing_spend = False
        missing_revenue = True
        merged_data = media_data_gma.groupby([date_col,'Gma']).sum().reset_index() #if not: use only media_data
      else: #media_data & revenue_data exist:
        merged_data = pd.merge(revenue_data_gma.groupby([date_col2,'Gma']).sum().reset_index(),media_data_gma.groupby([date_col,'Gma']).sum().reset_index(),  how=join_type, left_on=[date_col2,'Gma'], right_on = [date_col,'Gma'])
        missing_spend = False
        missing_revenue = False
        #keeping only 1 date column according to join_type:
        if date_col != date_col2:
          if join_type == "left":
            merged_data.drop(columns = date_col, inplace = True) #keep only date_col2 from revenue_data
          elif join_type == "right":
            merged_data.drop(columns = date_col2, inplace = True) #keep only date_col from media_data
          elif join_type == "inner":
            merged_data.drop(columns = date_col2, inplace = True) #keep only date col from media_data

  print("Number of geos in merged data: ")
  print(merged_data["Gma"].nunique())

  try:
    print("Share of response present in merge dataframe:")
    print(round(merged_data[response_col].sum() / init_revenue,4))
  except:
    pass

  try:
    print("Share of media present in merge dataframe:")
    print(round(merged_data[spend_col].sum() / init_spend,4))
  except:
    pass

else: #if geo_level = 'Gma': just merge revenue and media by geo_col & geo_col2
  revenue_data.loc[:, revenue_data.columns != date_col2] = revenue_data.loc[:, revenue_data.columns != date_col2].apply(pd.to_numeric, errors='coerce').fillna(revenue_data.loc[:, revenue_data.columns != date_col2])
  media_data.loc[:, media_data.columns != date_col] = media_data.loc[:, media_data.columns != date_col].apply(pd.to_numeric, errors='coerce').fillna(media_data.loc[:, media_data.columns != date_col])
  revenue_data_gma = revenue_data.groupby([date_col2, geo_col2]).sum().reset_index()
  media_data_gma = media_data.groupby([date_col, geo_col]).sum().reset_index()
  merged_data = pd.merge(revenue_data_gma, media_data_gma,  how=join_type, left_on=[date_col2, geo_col2], right_on = [date_col, geo_col])

#columns processing:
#removing columns that are supposed to be IDs
id_cols = [col for col in merged_data.columns if 'id' in col[-2:].lower()]
merged_data.drop(columns = id_cols, inplace=True)

#Standarzing column names:
merged_data.columns = ['date' if c == date_col else 'date' if c == date_col2 else 'geoName' if c == geo_col2 else 'geoName' if c == geo_col else 'geo' if c == 'Gma' else 'response' if c == response_col else 'cost' if c == spend_col else c for c in merged_data.columns]
#converting merged_data columns to num if possible
for col in merged_data.drop(['date','geo'], axis=1, errors='ignore').columns:
  try:
    merged_data[col] = pd.to_numeric(merged_data[col], errors='ignore')
  except:
    pass

try:
  print("\n")
  print(merged_data.head(3).to_markdown())
except:
  pass

#@title **b. Missing data handler**

try:
  assignment_table
except:
  assignment_table = pd.DataFrame()

#@markdown *Select method = None if handle you don't need to handle NaN then click on "Run Interact"*
user_form = interactive(fill_missing_data, {'manual': True},threshold=FloatSlider(min=0,value=0.75, max=1, step=0.05));
display(user_form)
#atexit.register(fill_missing_data)

#@title **c. Top / Bottom DMAs by spend**
try:
  processed_merged_data=user_form.result[0]
except:
  processed_merged_data = merged_data.copy()

freq="D"
col = 'cost'
# Top 5 DMAs with the highest average spend
print("                                                                 "+'\033[1m' + "Top " + '\033[0;0m' + "5 DMAs with the highest" + '\033[1m' + " average spend" + '\033[0;0m')
print("______________________________________________________________________________________________________________________________________________________________________________")
list1 = processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=False).head(5).index
fig = plt.figure(figsize=(23,5))
ax1 = fig.add_subplot(121)
sns.lineplot(data=processed_merged_data[processed_merged_data.geo.isin(list1)], x="date", y=processed_merged_data[processed_merged_data.geo.isin(list1)][col],hue='geo',ax=ax1)
ax2 = fig.add_subplot(122)
font_size=14
bbox=[0, 0, 1, 1]
ax2.axis('off')
mpl_table = ax2.table(cellText = pd.DataFrame(processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=False).head(5)).reset_index().values, bbox=bbox, colLabels=pd.DataFrame(processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=False).head(5)).reset_index().columns)
plt.show()
print('\n')
# Bottom 5 DMAs in average spend
#print('Bottom 5 DMAs in average spend')
print("                                                                 "+'\033[1m' + "Bottom " + '\033[0;0m' + "5 DMAs in" + '\033[1m' + " average spend" + '\033[0;0m')
print("______________________________________________________________________________________________________________________________________________________________________________")
list2 = processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=True).head(5).index
fig = plt.figure(figsize=(23,5))
ax1 = fig.add_subplot(121)
sns.lineplot(data=processed_merged_data[processed_merged_data.geo.isin(list2)], x="date", y=processed_merged_data[processed_merged_data.geo.isin(list2)][col],hue='geo',ax=ax1)
ax2 = fig.add_subplot(122)
font_size=14
bbox=[0, 0, 1, 1]
ax2.axis('off')
mpl_table = ax2.table(cellText = pd.DataFrame(processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=True).head(5)).reset_index().values, bbox=bbox, colLabels=pd.DataFrame(processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=True).head(5)).reset_index().columns)
plt.show()
print('\n')

# Top 5 DMAs with the highest total spend
print("                                                                 "+'\033[1m' + "Top " + '\033[0;0m' + "5 DMAs with the highest" + '\033[1m' + " total spend" + '\033[0;0m')
print("______________________________________________________________________________________________________________________________________________________________________________")
list3 = processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=False).head(5).index
fig = plt.figure(figsize=(23,5))
ax1 = fig.add_subplot(121)
sns.lineplot(data=processed_merged_data[processed_merged_data.geo.isin(list3)], x="date", y=processed_merged_data[processed_merged_data.geo.isin(list3)][col],hue='geo',ax=ax1)
ax2 = fig.add_subplot(122)
font_size=14
bbox=[0, 0, 1, 1]
ax2.axis('off')
mpl_table = ax2.table(cellText = pd.DataFrame(processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=False).head(5)).reset_index().values, bbox=bbox, colLabels=pd.DataFrame(processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=False).head(5)).reset_index().columns)
plt.show()
print("\n")

# Bottom 5 DMAs in total spend
#print('Bottom 5 DMAs in total spend')
print("                                                                 "+'\033[1m' + "Bottom " + '\033[0;0m' + "5 DMAs in" + '\033[1m' + " total spend" + '\033[0;0m')
print("______________________________________________________________________________________________________________________________________________________________________________")
list4 = processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=True).head(5).index
fig = plt.figure(figsize=(23,5))
ax1 = fig.add_subplot(121)
sns.lineplot(data=processed_merged_data[processed_merged_data.geo.isin(list4)], x="date", y=processed_merged_data[processed_merged_data.geo.isin(list4)][col],hue='geo',ax=ax1)
ax2 = fig.add_subplot(122)
font_size=14
bbox=[0, 0, 1, 1]
ax2.axis('off')
mpl_table = ax2.table(cellText = pd.DataFrame(processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=True).head(5)).reset_index().values, bbox=bbox, colLabels=pd.DataFrame(processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=True).head(5)).reset_index().columns)
plt.show()

#@title **d. Top / Bottom DMAs by revenue**
try:
  processed_merged_data=user_form.result[0]
except:
  processed_merged_data = merged_data.copy()

freq="D"
col = 'response'
# Top 5 DMAs with the highest average revenue
print("                                                                 "+'\033[1m' + "Top " + '\033[0;0m' + "5 DMAs with the highest" + '\033[1m' + " average revenue" + '\033[0;0m')
print("______________________________________________________________________________________________________________________________________________________________________________")
list1 = processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=False).head(5).index
fig = plt.figure(figsize=(23,5))
ax1 = fig.add_subplot(121)
sns.lineplot(data=processed_merged_data[processed_merged_data.geo.isin(list1)], x="date", y=processed_merged_data[processed_merged_data.geo.isin(list1)][col],hue='geo',ax=ax1)
ax2 = fig.add_subplot(122)
font_size=14
bbox=[0, 0, 1, 1]
ax2.axis('off')
mpl_table = ax2.table(cellText = pd.DataFrame(processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=False).head(5)).reset_index().values, bbox=bbox, colLabels=pd.DataFrame(processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=False).head(5)).reset_index().columns)
plt.show()
print('\n')
# Bottom 5 DMAs in average revenue
#print('Bottom 5 DMAs in average revenue')
print("                                                                 "+'\033[1m' + "Bottom " + '\033[0;0m' + "5 DMAs in" + '\033[1m' + " average revenue" + '\033[0;0m')
print("______________________________________________________________________________________________________________________________________________________________________________")
list2 = processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=True).head(5).index
fig = plt.figure(figsize=(23,5))
ax1 = fig.add_subplot(121)
sns.lineplot(data=processed_merged_data[processed_merged_data.geo.isin(list2)], x="date", y=processed_merged_data[processed_merged_data.geo.isin(list2)][col],hue='geo',ax=ax1)
ax2 = fig.add_subplot(122)
font_size=14
bbox=[0, 0, 1, 1]
ax2.axis('off')
mpl_table = ax2.table(cellText = pd.DataFrame(processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=True).head(5)).reset_index().values, bbox=bbox, colLabels=pd.DataFrame(processed_merged_data.groupby("geo")[col].mean().sort_values(ascending=True).head(5)).reset_index().columns)
plt.show()
print('\n')

# Top 5 DMAs with the highest total revenue
print("                                                                 "+'\033[1m' + "Top " + '\033[0;0m' + "5 DMAs with the highest" + '\033[1m' + " total revenue" + '\033[0;0m')
print("______________________________________________________________________________________________________________________________________________________________________________")
list3 = processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=False).head(5).index
fig = plt.figure(figsize=(23,5))
ax1 = fig.add_subplot(121)
sns.lineplot(data=processed_merged_data[processed_merged_data.geo.isin(list3)], x="date", y=processed_merged_data[processed_merged_data.geo.isin(list3)][col],hue='geo',ax=ax1)
ax2 = fig.add_subplot(122)
font_size=14
bbox=[0, 0, 1, 1]
ax2.axis('off')
mpl_table = ax2.table(cellText = pd.DataFrame(processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=False).head(5)).reset_index().values, bbox=bbox, colLabels=pd.DataFrame(processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=False).head(5)).reset_index().columns)
plt.show()
print("\n")

# Bottom 5 DMAs in total revenue
#print('Bottom 5 DMAs in total revenue')
print("                                                                 "+'\033[1m' + "Bottom " + '\033[0;0m' + "5 DMAs in" + '\033[1m' + " total revenue" + '\033[0;0m')
print("______________________________________________________________________________________________________________________________________________________________________________")
list4 = processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=True).head(5).index
fig = plt.figure(figsize=(23,5))
ax1 = fig.add_subplot(121)
sns.lineplot(data=processed_merged_data[processed_merged_data.geo.isin(list4)], x="date", y=processed_merged_data[processed_merged_data.geo.isin(list4)][col],hue='geo',ax=ax1)
ax2 = fig.add_subplot(122)
font_size=14
bbox=[0, 0, 1, 1]
ax2.axis('off')
mpl_table = ax2.table(cellText = pd.DataFrame(processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=True).head(5)).reset_index().values, bbox=bbox, colLabels=pd.DataFrame(processed_merged_data.groupby("geo")[col].sum().sort_values(ascending=True).head(5)).reset_index().columns)
plt.show()

"""---
<img src="https://drive.google.com/uc?id=1AJnSLScn3kSrOLPZGONcfIJ4m8ZnyHmR" height="50px" align="left" hspace="4px" alt="New"></img>
# **V. EXPORT**
---
"""

#@title **Create spreadsheet**

try:
  processed_merged_data=user_form.result[0]
except:
  processed_merged_data = merged_data.copy()

spreadsheet_body = {
    'properties': {
        'title': "geolift_"+analysis_name+"_"+adv_name+"_"+da_name+"_data"
    },
    'sheets': [{
        'properties' : {
            'title' : 'geodata',
            'tabColor': {
                'red': 0,
                'green': 0,
                'blue': 0.70
            }
        }
    }]
}

sh3 = service_sheet.spreadsheets().create(body=spreadsheet_body).execute()
spreadsheetId = sh3['spreadsheetId']
print(f"New spreadsheet created with id:{spreadsheetId} and url:{sh3['spreadsheetUrl']}")

# insert data
try:
  #processed_merged_data["date"] = processed_merged_data["date"].astype(str)
  insert_data(processed_merged_data.fillna(0).astype(str), spreadsheetId, 'geodata','1','A')
except:
  print("Error: Try to run 'IV. b. Merged data processing (optionnal)' with < method = 'None' > if you don't need the merged data to be processed.")

dict_rgb_grey = { 'r': 0.79, 'g': 0.79, 'b': 0.79 }

if fuzzy_matching == 'Yes':
  #add sheet with list of matched locations (from revenue data) with their corresponding GMAs
  try:
    add_sheets(spreadsheetId, 'check_gma_revenue', dict_rgb_grey)
    insert_data(df.fillna(0).astype(str), spreadsheetId, 'check_gma_revenue','1','A')
  except:
    pass
  #add sheet with list of matched locations (from spend data) with their corresponding GMAs
  try:
    add_sheets(spreadsheetId, 'check_gma_media', dict_rgb_grey)
    insert_data(df2.fillna(0).astype(str), spreadsheetId, 'check_gma_media','1','A')
  except:
    pass